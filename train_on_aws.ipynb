{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'cmajorsolo-autoformer-data'\n",
    "file_name = 'exchange_rate_short.csv'\n",
    "s3_data_distribution_type = 'FullyReplicated'\n",
    "data_url = \"https://cmajorsolo-autoformer-data.s3.eu-west-1.amazonaws.com/exchange_rate_short.csv\"\n",
    "data_s3_uri = 's3://{}/{}'.format(bucket, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model with Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build docker container for the train job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "ecr_repository = 'hz_first_test'\n",
    "tag = 'latest'\n",
    "processing_repository_uri = '{}.dkr.ecr.{}.amazonaws.com/{}'.format(account_id, region, ecr_repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                                         \n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.1s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  0.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.4s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  0.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.7s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  0.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.9s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  0.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.0s (2/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  0.9s\n",
      " => [auth] pytorch/pytorch:pull token for registry-1.docker.io             0.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.2s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  1.0s\n",
      "\u001b[34m => [auth] pytorch/pytorch:pull token for registry-1.docker.io             0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.3s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  1.2s\n",
      "\u001b[34m => [auth] pytorch/pytorch:pull token for registry-1.docker.io             0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.5s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  1.3s\n",
      "\u001b[34m => [auth] pytorch/pytorch:pull token for registry-1.docker.io             0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.6s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  1.5s\n",
      "\u001b[34m => [auth] pytorch/pytorch:pull token for registry-1.docker.io             0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.7s (8/8) FINISHED                                                \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  1.5s\n",
      "\u001b[0m\u001b[34m => [auth] pytorch/pytorch:pull token for registry-1.docker.io             0.0s\n",
      "\u001b[0m\u001b[34m => [1/3] FROM docker.io/pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime@sh  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/3] RUN apt-get update && apt-get install gcc -y              0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/3] RUN pip install sagemaker-pytorch-training                0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:284910e3b33e19ad0581ff4efac206fdf094053517520  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/hz_first_test                           0.0s\n",
      "\u001b[0m\u001b[?25h\n",
      "Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $ecr_repository ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload image to AWS ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "The push refers to repository [291287855072.dkr.ecr.eu-west-1.amazonaws.com/hz_first_test]\n",
      "\n",
      "\u001b[1B38bb0a16: Preparing \n",
      "\u001b[1Bc34122fd: Preparing \n",
      "\u001b[1B732c9258: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1B6af2f4ef: Preparing \n",
      "\u001b[1B6e8c217d: Preparing \n",
      "\u001b[1Bca73c74f: Waiting g denied: Your authorization token has expired. Reauthenticate and try again.\n"
     ]
    }
   ],
   "source": [
    "# !aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {account_id}.dkr.ecr.{region}.amazonaws.com\n",
    "!docker tag {ecr_repository} $processing_repository_uri\n",
    "!docker push $processing_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model with the customised image created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "\n",
    "execution_role = \"AmazonSageMaker-ExecutionRole-20210905T154857\"\n",
    "custom_image_uri = \"291287855072.dkr.ecr.eu-west-1.amazonaws.com/hz_first_test:latest\"\n",
    "# source_dirs = [\"data_provider\", \"dataset\", \"exp\", \"layers\", \"models\", \"utils\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: autoformer-training-job-2023-08-29-22-00-35-785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-29 22:01:59 Starting - Starting the training job...\n",
      "2023-08-29 22:02:25 Starting - Preparing the instances for training.........\n",
      "2023-08-29 22:03:57 Downloading - Downloading input data\n",
      "2023-08-29 22:03:57 Training - Downloading the training image.....................\n",
      "2023-08-29 22:07:29 Training - Training image download completed. Training in progress....bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2023-08-29 22:07:50,192 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2023-08-29 22:07:50,225 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2023-08-29 22:07:50,228 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2023-08-29 22:07:52,579 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "/opt/conda/bin/python3.6 -m pip install -r requirements.txt\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.1.5)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.24.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (1.19.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (3.3.4)\n",
      "Collecting reformer_pytorch\n",
      "  Downloading reformer_pytorch-1.4.4-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 1)) (2021.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->-r requirements.txt (line 2)) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.5.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.6/site-packages (from matplotlib->-r requirements.txt (line 4)) (8.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->-r requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.6/site-packages (from matplotlib->-r requirements.txt (line 4)) (3.0.6)\n",
      "Collecting einops\n",
      "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
      "Collecting local-attention\n",
      "  Downloading local_attention-1.8.6-py3-none-any.whl (8.1 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from reformer_pytorch->-r requirements.txt (line 5)) (1.8.1)\n",
      "Collecting product-key-memory\n",
      "  Downloading product_key_memory-0.2.9-py3-none-any.whl (6.2 kB)\n",
      "Collecting axial-positional-embedding>=0.1.0\n",
      "  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 1)) (1.16.0)\n",
      "Collecting local-attention\n",
      "  Downloading local_attention-1.8.5-py3-none-any.whl (8.1 kB)\n",
      "  Downloading local_attention-1.8.4-py3-none-any.whl (8.0 kB)\n",
      "  Downloading local_attention-1.8.2-py3-none-any.whl (7.9 kB)\n",
      "  Downloading local_attention-1.8.1-py3-none-any.whl (8.0 kB)\n",
      "  Downloading local_attention-1.8.0-py3-none-any.whl (7.9 kB)\n",
      "  Downloading local_attention-1.7.2-py3-none-any.whl (7.5 kB)\n",
      "  Downloading local_attention-1.7.1-py3-none-any.whl (7.4 kB)\n",
      "  Downloading local_attention-1.7.0-py3-none-any.whl (7.4 kB)\n",
      "  Downloading local_attention-1.6.0-py3-none-any.whl (7.0 kB)\n",
      "  Downloading local_attention-1.5.8-py3-none-any.whl (6.9 kB)\n",
      "  Downloading local_attention-1.5.7-py3-none-any.whl (6.9 kB)\n",
      "  Downloading local_attention-1.5.6-py3-none-any.whl (6.9 kB)\n",
      "  Downloading local_attention-1.5.5-py3-none-any.whl (6.8 kB)\n",
      "  Downloading local_attention-1.5.4-py3-none-any.whl (6.8 kB)\n",
      "  Downloading local_attention-1.5.3-py3-none-any.whl (6.8 kB)\n",
      "  Downloading local_attention-1.5.2-py3-none-any.whl (6.8 kB)\n",
      "  Downloading local_attention-1.5.1-py3-none-any.whl (6.8 kB)\n",
      "  Downloading local_attention-1.5.0-py3-none-any.whl (6.7 kB)\n",
      "  Downloading local_attention-1.4.4-py3-none-any.whl (5.0 kB)\n",
      "Collecting colt5-attention>=0.10.14\n",
      "  Downloading CoLT5_attention-0.10.15-py3-none-any.whl (18 kB)\n",
      "Collecting product-key-memory\n",
      "  Downloading product_key_memory-0.2.8-py3-none-any.whl (4.5 kB)\n",
      "  Downloading product_key_memory-0.2.7-py3-none-any.whl (4.4 kB)\n",
      "  Downloading product_key_memory-0.2.6-py3-none-any.whl (4.4 kB)\n",
      "  Downloading product_key_memory-0.2.5-py3-none-any.whl (4.4 kB)\n",
      "  Downloading product_key_memory-0.2.4-py3-none-any.whl (4.3 kB)\n",
      "  Downloading product_key_memory-0.2.3-py3-none-any.whl (4.3 kB)\n",
      "  Downloading product_key_memory-0.2.2-py3-none-any.whl (4.1 kB)\n",
      "  Downloading product_key_memory-0.2.1-py3-none-any.whl (4.1 kB)\n",
      "  Downloading product_key_memory-0.2.0-py3-none-any.whl (4.1 kB)\n",
      "  Downloading product_key_memory-0.1.11-py3-none-any.whl (4.0 kB)\n",
      "  Downloading product_key_memory-0.1.10.tar.gz (3.5 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch->reformer_pytorch->-r requirements.txt (line 5)) (3.10.0.2)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch->reformer_pytorch->-r requirements.txt (line 5)) (0.8)\n",
      "Building wheels for collected packages: axial-positional-embedding, product-key-memory\n",
      "  Building wheel for axial-positional-embedding (setup.py): started\n",
      "  Building wheel for axial-positional-embedding (setup.py): finished with status 'done'\n",
      "  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2902 sha256=e6d4a91aa2d06910bd1ceaed4bb37e119ccfe49556d58dc95c9c53e38554a986\n",
      "  Stored in directory: /root/.cache/pip/wheels/87/1e/cd/b5de135ee3faf0c4c525553227e601b0e7739264014dc3e98f\n",
      "  Building wheel for product-key-memory (setup.py): started\n",
      "  Building wheel for product-key-memory (setup.py): finished with status 'done'\n",
      "  Created wheel for product-key-memory: filename=product_key_memory-0.1.10-py3-none-any.whl size=3071 sha256=295e3f297f3d229750aca2017f136f163f24560f207d7a1b779ce53da26928ba\n",
      "  Stored in directory: /root/.cache/pip/wheels/a0/fc/1f/ecdffc3d221abee79cac215c986fb1913ae09c23e4705d86cc\n",
      "Successfully built axial-positional-embedding product-key-memory\n",
      "Installing collected packages: product-key-memory, local-attention, einops, axial-positional-embedding, reformer-pytorch\n",
      "Successfully installed axial-positional-embedding-0.2.1 einops-0.4.1 local-attention-1.4.4 product-key-memory-0.1.10 reformer-pytorch-1.4.4\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "2023-08-29 22:07:58,538 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"autoformer-training-job-2023-08-29-22-00-35-785\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-291287855072/autoformer-training-job-2023-08-29-22-00-35-785/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={}\n",
      "SM_USER_ENTRY_POINT=run.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"train\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_MODULE_NAME=run\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=8\n",
      "SM_NUM_GPUS=1\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-eu-west-1-291287855072/autoformer-training-job-2023-08-29-22-00-35-785/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"autoformer-training-job-2023-08-29-22-00-35-785\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-291287855072/autoformer-training-job-2023-08-29-22-00-35-785/source/sourcedir.tar.gz\",\"module_name\":\"run\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run.py\"}\n",
      "SM_USER_ARGS=[]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "Invoking script with the following command:\n",
      "/opt/conda/bin/python3.6 run.py\n",
      "Args in experiment:\n",
      "Namespace(activation='gelu', batch_size=32, bucket_size=4, c_out=8, checkpoints='checkpoints/', d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='exchange_rate_short.csv', dec_in=8, des='Exp', devices='0,1,2,3', distil=True, do_predict=True, dropout=0.05, e_layers=2, embed='timeF', enc_in=8, factor=3, features='M', freq='h', gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, loss='mse', lradj='type1', model='Transformer', model_id='Exchange_96_96', moving_avg=25, n_hashes=4, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='/opt/ml/input/data/train', seq_len=96, sm_model_dir='/opt/ml/model', target='OT', train_epochs=1, use_amp=False, use_gpu=True, use_multi_gpu=False)\n",
      ">>>>>>> data rooth path : /opt/ml/input/data/train>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>> data file path : exchange_rate_short.csv>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : Exchange_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1343\n",
      "val 125\n",
      "test 343\n",
      "[2023-08-29 22:08:03.572 algo-1:53 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-08-29 22:08:03.616 algo-1:53 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "[2023-08-29 22:08:03.616 algo-1:53 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-08-29 22:08:03.617 algo-1:53 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-08-29 22:08:03.617 algo-1:53 INFO hook.py:255] Saving to /opt/ml/output/tensors\n",
      "[2023-08-29 22:08:03.618 algo-1:53 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-08-29 22:08:04.659 algo-1:53 INFO hook.py:591] name:enc_embedding.value_embedding.tokenConv.weight count_params:12288\n",
      "[2023-08-29 22:08:04.659 algo-1:53 INFO hook.py:591] name:enc_embedding.temporal_embedding.embed.weight count_params:2048\n",
      "[2023-08-29 22:08:04.660 algo-1:53 INFO hook.py:591] name:dec_embedding.value_embedding.tokenConv.weight count_params:12288\n",
      "[2023-08-29 22:08:04.660 algo-1:53 INFO hook.py:591] name:dec_embedding.temporal_embedding.embed.weight count_params:2048\n",
      "[2023-08-29 22:08:04.660 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.0.attention.query_projection.weight count_params:262144\n",
      "[2023-08-29 22:08:04.660 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.0.attention.query_projection.bias count_params:512\n",
      "[2023-08-29 22:08:04.660 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.0.attention.key_projection.weight count_params:262144\n",
      "[2023-08-29 22:08:04.660 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.0.attention.key_projection.bias count_params:512\n",
      "[2023-08-29 22:08:04.660 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.0.attention.value_projection.weight count_params:262144\n",
      "[2023-08-29 22:08:04.660 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.0.attention.value_projection.bias count_params:512\n",
      "[2023-08-29 22:08:04.660 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.0.attention.out_projection.weight count_params:262144\n",
      "[2023-08-29 22:08:04.660 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.0.attention.out_projection.bias count_params:512\n",
      "[2023-08-29 22:08:04.660 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.0.conv1.weight count_params:1048576\n",
      "[2023-08-29 22:08:04.660 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.0.conv1.bias count_params:2048\n",
      "[2023-08-29 22:08:04.661 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.0.conv2.weight count_params:1048576\n",
      "[2023-08-29 22:08:04.661 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.0.conv2.bias count_params:512\n",
      "[2023-08-29 22:08:04.661 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.0.norm1.weight count_params:512\n",
      "[2023-08-29 22:08:04.661 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.0.norm1.bias count_params:512\n",
      "[2023-08-29 22:08:04.661 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.0.norm2.weight count_params:512\n",
      "[2023-08-29 22:08:04.661 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.0.norm2.bias count_params:512\n",
      "[2023-08-29 22:08:04.661 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.1.attention.query_projection.weight count_params:262144\n",
      "[2023-08-29 22:08:04.661 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.1.attention.query_projection.bias count_params:512\n",
      "[2023-08-29 22:08:04.661 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.1.attention.key_projection.weight count_params:262144\n",
      "[2023-08-29 22:08:04.661 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.1.attention.key_projection.bias count_params:512\n",
      "[2023-08-29 22:08:04.661 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.1.attention.value_projection.weight count_params:262144\n",
      "[2023-08-29 22:08:04.661 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.1.attention.value_projection.bias count_params:512\n",
      "[2023-08-29 22:08:04.662 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.1.attention.out_projection.weight count_params:262144\n",
      "[2023-08-29 22:08:04.662 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.1.attention.out_projection.bias count_params:512\n",
      "[2023-08-29 22:08:04.662 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.1.conv1.weight count_params:1048576\n",
      "[2023-08-29 22:08:04.662 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.1.conv1.bias count_params:2048\n",
      "[2023-08-29 22:08:04.662 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.1.conv2.weight count_params:1048576\n",
      "[2023-08-29 22:08:04.662 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.1.conv2.bias count_params:512\n",
      "[2023-08-29 22:08:04.662 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.1.norm1.weight count_params:512\n",
      "[2023-08-29 22:08:04.662 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.1.norm1.bias count_params:512\n",
      "[2023-08-29 22:08:04.662 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.1.norm2.weight count_params:512\n",
      "[2023-08-29 22:08:04.662 algo-1:53 INFO hook.py:591] name:encoder.attn_layers.1.norm2.bias count_params:512\n",
      "[2023-08-29 22:08:04.662 algo-1:53 INFO hook.py:591] name:encoder.norm.weight count_params:512\n",
      "[2023-08-29 22:08:04.662 algo-1:53 INFO hook.py:591] name:encoder.norm.bias count_params:512\n",
      "[2023-08-29 22:08:04.663 algo-1:53 INFO hook.py:591] name:decoder.layers.0.self_attention.query_projection.weight count_params:262144\n",
      "[2023-08-29 22:08:04.663 algo-1:53 INFO hook.py:591] name:decoder.layers.0.self_attention.query_projection.bias count_params:512\n",
      "[2023-08-29 22:08:04.663 algo-1:53 INFO hook.py:591] name:decoder.layers.0.self_attention.key_projection.weight count_params:262144\n",
      "[2023-08-29 22:08:04.663 algo-1:53 INFO hook.py:591] name:decoder.layers.0.self_attention.key_projection.bias count_params:512\n",
      "[2023-08-29 22:08:04.663 algo-1:53 INFO hook.py:591] name:decoder.layers.0.self_attention.value_projection.weight count_params:262144\n",
      "[2023-08-29 22:08:04.663 algo-1:53 INFO hook.py:591] name:decoder.layers.0.self_attention.value_projection.bias count_params:512\n",
      "[2023-08-29 22:08:04.663 algo-1:53 INFO hook.py:591] name:decoder.layers.0.self_attention.out_projection.weight count_params:262144\n",
      "[2023-08-29 22:08:04.663 algo-1:53 INFO hook.py:591] name:decoder.layers.0.self_attention.out_projection.bias count_params:512\n",
      "[2023-08-29 22:08:04.663 algo-1:53 INFO hook.py:591] name:decoder.layers.0.cross_attention.query_projection.weight count_params:262144\n",
      "[2023-08-29 22:08:04.663 algo-1:53 INFO hook.py:591] name:decoder.layers.0.cross_attention.query_projection.bias count_params:512\n",
      "[2023-08-29 22:08:04.663 algo-1:53 INFO hook.py:591] name:decoder.layers.0.cross_attention.key_projection.weight count_params:262144\n",
      "[2023-08-29 22:08:04.663 algo-1:53 INFO hook.py:591] name:decoder.layers.0.cross_attention.key_projection.bias count_params:512\n",
      "[2023-08-29 22:08:04.664 algo-1:53 INFO hook.py:591] name:decoder.layers.0.cross_attention.value_projection.weight count_params:262144\n",
      "[2023-08-29 22:08:04.664 algo-1:53 INFO hook.py:591] name:decoder.layers.0.cross_attention.value_projection.bias count_params:512\n",
      "[2023-08-29 22:08:04.664 algo-1:53 INFO hook.py:591] name:decoder.layers.0.cross_attention.out_projection.weight count_params:262144\n",
      "[2023-08-29 22:08:04.664 algo-1:53 INFO hook.py:591] name:decoder.layers.0.cross_attention.out_projection.bias count_params:512\n",
      "[2023-08-29 22:08:04.664 algo-1:53 INFO hook.py:591] name:decoder.layers.0.conv1.weight count_params:1048576\n",
      "[2023-08-29 22:08:04.664 algo-1:53 INFO hook.py:591] name:decoder.layers.0.conv1.bias count_params:2048\n",
      "[2023-08-29 22:08:04.664 algo-1:53 INFO hook.py:591] name:decoder.layers.0.conv2.weight count_params:1048576\n",
      "[2023-08-29 22:08:04.664 algo-1:53 INFO hook.py:591] name:decoder.layers.0.conv2.bias count_params:512\n",
      "[2023-08-29 22:08:04.664 algo-1:53 INFO hook.py:591] name:decoder.layers.0.norm1.weight count_params:512\n",
      "[2023-08-29 22:08:04.664 algo-1:53 INFO hook.py:591] name:decoder.layers.0.norm1.bias count_params:512\n",
      "[2023-08-29 22:08:04.664 algo-1:53 INFO hook.py:591] name:decoder.layers.0.norm2.weight count_params:512\n",
      "[2023-08-29 22:08:04.664 algo-1:53 INFO hook.py:591] name:decoder.layers.0.norm2.bias count_params:512\n",
      "[2023-08-29 22:08:04.665 algo-1:53 INFO hook.py:591] name:decoder.layers.0.norm3.weight count_params:512\n",
      "[2023-08-29 22:08:04.665 algo-1:53 INFO hook.py:591] name:decoder.layers.0.norm3.bias count_params:512\n",
      "[2023-08-29 22:08:04.665 algo-1:53 INFO hook.py:591] name:decoder.norm.weight count_params:512\n",
      "[2023-08-29 22:08:04.665 algo-1:53 INFO hook.py:591] name:decoder.norm.bias count_params:512\n",
      "[2023-08-29 22:08:04.665 algo-1:53 INFO hook.py:591] name:decoder.projection.weight count_params:4096\n",
      "[2023-08-29 22:08:04.665 algo-1:53 INFO hook.py:591] name:decoder.projection.bias count_params:8\n",
      "[2023-08-29 22:08:04.665 algo-1:53 INFO hook.py:593] Total Trainable Params: 10543624\n",
      "[2023-08-29 22:08:04.665 algo-1:53 INFO hook.py:425] Monitoring the collections: losses\n",
      "[2023-08-29 22:08:04.669 algo-1:53 INFO hook.py:488] Hook is writing from the hook with pid: 53\n",
      "Epoch: 1 cost time: 4.750582933425903\n",
      "Epoch: 1, Steps: 41 | Train Loss: 0.2320563 Vali Loss: 0.5791586 Test Loss: 1.7432882\n",
      "Validation loss decreased (inf --> 0.579159).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : Exchange_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 343\n",
      "test shape: (343, 96, 8) (343, 96, 8)\n",
      "test shape: (343, 96, 8) (343, 96, 8)\n",
      "mse:1.7365827560424805, mae:1.0237808227539062\n",
      ">>>>>>>predicting : Exchange_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "pred 1\n",
      "2023-08-29 22:08:03,096 NumExpr defaulting to 8 threads.\n",
      "2023-08-29 22:08:03,131 setting is Exchange_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0\n",
      "2023-08-29 22:08:03,131 save model to /opt/ml/model/checkpoints/Exchange_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0\n",
      "2023-08-29 22:08:08,962 loading model from /opt/ml/model/checkpoints/Exchange_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0/checkpoint.pth\n",
      "2023-08-29 22:08:09,018 Testing: sm_model_dir is /opt/ml/model\n",
      "2023-08-29 22:08:09,018 Testing: Test result will be saved into folder /opt/ml/model/./test_results/Exchange_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0\n",
      "2023-08-29 22:08:09,877 Testing: Result will be saved into folder /opt/ml/model/./results/Exchange_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0\n",
      "2023-08-29 22:08:09,910 loading model from /opt/ml/model/checkpoints/Exchange_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0\n",
      "2023-08-29 22:08:10,346 Preding: Pred result will be saved into folder /opt/ml/model/./predResults/Exchange_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0\n",
      "2023-08-29 22:08:11,130 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\n",
      "2023-08-29 22:08:30 Uploading - Uploading generated training model\n",
      "2023-08-29 22:08:30 Completed - Training job completed\n",
      "Training seconds: 293\n",
      "Billable seconds: 293\n"
     ]
    }
   ],
   "source": [
    "#Create the estimator object for PyTorch\n",
    "import os\n",
    "from sagemaker.pytorch.estimator import PyTorch # import PyTorch Estimator class \n",
    "\n",
    "estimator = PyTorch(\n",
    "    # Use the image created and pushed in the previous steps\n",
    "    # image_uri=custom_image_uri, #our custom pytorch image URI\n",
    "    entry_point = \"run_aws.py\", # training script\n",
    "    # Below two params are used with the default pytorch image that built by SageMaker\n",
    "    framework_version = \"1.8.1\", #PyTorch version\n",
    "    py_version = \"py3\", # Compatible Python version to use\n",
    "    instance_count = 1, #number of EC2 instances needed for training\n",
    "    # instance_type = \"ml.c5.xlarge\", #Type of EC2 instance/s needed for training\n",
    "    instance_type = \"ml.p3.2xlarge\", #Type of EC2 instance with GPU needed for training\n",
    "    disable_profiler = True, #Disable profiler, as it's not needed\n",
    "    role = execution_role, #Execution role used by training job\n",
    "    source_dir = \"./\", #Directory where training script is located\n",
    "    base_job_name='autoformer-training-job', #Name of training job on AWS   \n",
    "    hyperparameters={}\n",
    ")\n",
    "\n",
    "s3_input_train = sagemaker.TrainingInput(s3_data=data_s3_uri, content_type='csv')\n",
    "\n",
    "inputs = {\"train\":s3_input_train}\n",
    "\n",
    "#Start the training in the ephemeral remote compute \n",
    "estimator.fit(inputs, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDos: \n",
    "1. Save the trained model into S3 bucket  \n",
    "    - Done by adding sm-model-dir in the parameter in run.py\n",
    "2. Use GPU to train \n",
    "    - Done by changing the estimator attribute to: instance_type = \"ml.p3.2xlarge\", #Type of EC2 instance with GPU needed for training\n",
    "3. Print out the test charts\n",
    "    - Done by updating folder_path params in exp_main.py\n",
    "4. Get BTC data running with Autoformer\n",
    "5. Set up early stop on epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy model with SageMaker\n",
    "# test model with SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize boto3 clients\n",
    "s3 = boto3.resource('s3')\n",
    "sagemaker = boto3.client('sagemaker')\n",
    "logs = boto3.client('logs')\n",
    "ecr = boto3.client('ecr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove S3 artifacts\n",
    "# bucket_name = 'sagemaker-eu-west-1-291287855072'\n",
    "# prefix = 'autoformer-training-job'  # Prefix to narrow down to specific files/directories\n",
    "# bucket = s3.Bucket(bucket_name)\n",
    "# for obj in bucket.objects.filter(Prefix=prefix):    \n",
    "#     pritn(\"Deleting S3 object: \"+obj.key)\n",
    "#     obj.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stop SageMaker notebook instances\n",
    "# notebook_instance_name = 'YOUR_NOTEBOOK_INSTANCE_NAME'\n",
    "# sagemaker.stop_notebook_instance(NotebookInstanceName=notebook_instance_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete CloudWatch logs\n",
    "# log_group_name = '/aws/sagemaker/TrainingJobs'\n",
    "# response = logs.describe_log_streams(logGroupName=log_group_name, orderBy='LastEventTime', descending=True)\n",
    "# for log_stream in response['logStreams']:\n",
    "#     if(log_stream['logStreamName'].startswith('autoformer-training-job')):\n",
    "#         print(\"Deleting log: \"+log_stream['logStreamName'])\n",
    "#         logs.delete_log_stream(logGroupName=log_group_name, logStreamName=log_stream['logStreamName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete Docker images from ECR\n",
    "# repository_name = 'hz_first_test'\n",
    "# images = ecr.list_images(repositoryName=repository_name)\n",
    "# for image in images.get('imageIds', []):\n",
    "#     if imageTag == \"latest\":\n",
    "#         # ecr.batch_delete_image(repositoryName=repository_name, imageIds=[image])\n",
    "#         print(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_train_aws_docker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
