{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'cmajorsolo-autoformer-data'\n",
    "file_name = 'BTC_full_1day.csv'\n",
    "s3_data_distribution_type = 'FullyReplicated'\n",
    "# data_url = \"https://cmajorsolo-autoformer-data.s3.eu-west-1.amazonaws.com/exchange_rate_short.csv\"\n",
    "data_s3_uri = 's3://{}/{}'.format(bucket, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model with Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build docker container for the train job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "ecr_repository = 'hz_first_test'\n",
    "tag = 'latest'\n",
    "processing_repository_uri = '{}.dkr.ecr.{}.amazonaws.com/{}'.format(account_id, region, ecr_repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                                         \n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.1s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  0.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.4s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  0.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.7s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  0.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.9s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  0.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.0s (2/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  0.9s\n",
      " => [auth] pytorch/pytorch:pull token for registry-1.docker.io             0.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.2s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  1.0s\n",
      "\u001b[34m => [auth] pytorch/pytorch:pull token for registry-1.docker.io             0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.3s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  1.2s\n",
      "\u001b[34m => [auth] pytorch/pytorch:pull token for registry-1.docker.io             0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.5s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  1.3s\n",
      "\u001b[34m => [auth] pytorch/pytorch:pull token for registry-1.docker.io             0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.6s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  1.5s\n",
      "\u001b[34m => [auth] pytorch/pytorch:pull token for registry-1.docker.io             0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.7s (8/8) FINISHED                                                \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/pytorch/pytorch:2.0.1-cuda11.7  1.5s\n",
      "\u001b[0m\u001b[34m => [auth] pytorch/pytorch:pull token for registry-1.docker.io             0.0s\n",
      "\u001b[0m\u001b[34m => [1/3] FROM docker.io/pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime@sh  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/3] RUN apt-get update && apt-get install gcc -y              0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/3] RUN pip install sagemaker-pytorch-training                0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:284910e3b33e19ad0581ff4efac206fdf094053517520  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/hz_first_test                           0.0s\n",
      "\u001b[0m\u001b[?25h\n",
      "Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $ecr_repository ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload image to AWS ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "The push refers to repository [291287855072.dkr.ecr.eu-west-1.amazonaws.com/hz_first_test]\n",
      "\n",
      "\u001b[1B38bb0a16: Preparing \n",
      "\u001b[1Bc34122fd: Preparing \n",
      "\u001b[1B732c9258: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1B6af2f4ef: Preparing \n",
      "\u001b[1B6e8c217d: Preparing \n",
      "\u001b[1Bca73c74f: Waiting g denied: Your authorization token has expired. Reauthenticate and try again.\n"
     ]
    }
   ],
   "source": [
    "# !aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {account_id}.dkr.ecr.{region}.amazonaws.com\n",
    "!docker tag {ecr_repository} $processing_repository_uri\n",
    "!docker push $processing_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model with the customised image created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "\n",
    "execution_role = \"AmazonSageMaker-ExecutionRole-20210905T154857\"\n",
    "custom_image_uri = \"291287855072.dkr.ecr.eu-west-1.amazonaws.com/hz_first_test:latest\"\n",
    "# source_dirs = [\"data_provider\", \"dataset\", \"exp\", \"layers\", \"models\", \"utils\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: transformer-btc-1d-96-96-2023-09-23-18-40-32-313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-23 18:44:33 Starting - Starting the training job...\n",
      "2023-09-23 18:44:57 Starting - Preparing the instances for training......\n",
      "2023-09-23 18:46:02 Downloading - Downloading input data...\n",
      "2023-09-23 18:46:27 Training - Downloading the training image.....................\n",
      "2023-09-23 18:50:03 Training - Training image download completed. Training in progress....bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2023-09-23 18:50:25,950 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2023-09-23 18:50:25,980 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2023-09-23 18:50:25,982 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2023-09-23 18:50:38,925 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "/opt/conda/bin/python3.6 -m pip install -r requirements.txt\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.1.5)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.24.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (1.19.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (3.3.4)\n",
      "Collecting reformer_pytorch\n",
      "  Downloading reformer_pytorch-1.4.4-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 1)) (2021.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.5.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->-r requirements.txt (line 2)) (2.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.6/site-packages (from matplotlib->-r requirements.txt (line 4)) (8.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib->-r requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.6/site-packages (from matplotlib->-r requirements.txt (line 4)) (3.0.6)\n",
      "Collecting local-attention\n",
      "  Downloading local_attention-1.8.6-py3-none-any.whl (8.1 kB)\n",
      "Collecting product-key-memory\n",
      "  Downloading product_key_memory-0.2.10-py3-none-any.whl (6.4 kB)\n",
      "Collecting axial-positional-embedding>=0.1.0\n",
      "  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting einops\n",
      "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from reformer_pytorch->-r requirements.txt (line 5)) (1.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 1)) (1.16.0)\n",
      "Collecting local-attention\n",
      "  Downloading local_attention-1.8.5-py3-none-any.whl (8.1 kB)\n",
      "  Downloading local_attention-1.8.4-py3-none-any.whl (8.0 kB)\n",
      "  Downloading local_attention-1.8.2-py3-none-any.whl (7.9 kB)\n",
      "  Downloading local_attention-1.8.1-py3-none-any.whl (8.0 kB)\n",
      "  Downloading local_attention-1.8.0-py3-none-any.whl (7.9 kB)\n",
      "  Downloading local_attention-1.7.2-py3-none-any.whl (7.5 kB)\n",
      "  Downloading local_attention-1.7.1-py3-none-any.whl (7.4 kB)\n",
      "  Downloading local_attention-1.7.0-py3-none-any.whl (7.4 kB)\n",
      "  Downloading local_attention-1.6.0-py3-none-any.whl (7.0 kB)\n",
      "  Downloading local_attention-1.5.8-py3-none-any.whl (6.9 kB)\n",
      "  Downloading local_attention-1.5.7-py3-none-any.whl (6.9 kB)\n",
      "  Downloading local_attention-1.5.6-py3-none-any.whl (6.9 kB)\n",
      "  Downloading local_attention-1.5.5-py3-none-any.whl (6.8 kB)\n",
      "  Downloading local_attention-1.5.4-py3-none-any.whl (6.8 kB)\n",
      "  Downloading local_attention-1.5.3-py3-none-any.whl (6.8 kB)\n",
      "  Downloading local_attention-1.5.2-py3-none-any.whl (6.8 kB)\n",
      "  Downloading local_attention-1.5.1-py3-none-any.whl (6.8 kB)\n",
      "  Downloading local_attention-1.5.0-py3-none-any.whl (6.7 kB)\n",
      "  Downloading local_attention-1.4.4-py3-none-any.whl (5.0 kB)\n",
      "Collecting product-key-memory\n",
      "  Downloading product_key_memory-0.2.9-py3-none-any.whl (6.2 kB)\n",
      "  Downloading product_key_memory-0.2.8-py3-none-any.whl (4.5 kB)\n",
      "  Downloading product_key_memory-0.2.7-py3-none-any.whl (4.4 kB)\n",
      "  Downloading product_key_memory-0.2.6-py3-none-any.whl (4.4 kB)\n",
      "  Downloading product_key_memory-0.2.5-py3-none-any.whl (4.4 kB)\n",
      "  Downloading product_key_memory-0.2.4-py3-none-any.whl (4.3 kB)\n",
      "  Downloading product_key_memory-0.2.3-py3-none-any.whl (4.3 kB)\n",
      "  Downloading product_key_memory-0.2.2-py3-none-any.whl (4.1 kB)\n",
      "  Downloading product_key_memory-0.2.1-py3-none-any.whl (4.1 kB)\n",
      "  Downloading product_key_memory-0.2.0-py3-none-any.whl (4.1 kB)\n",
      "  Downloading product_key_memory-0.1.11-py3-none-any.whl (4.0 kB)\n",
      "  Downloading product_key_memory-0.1.10.tar.gz (3.5 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch->reformer_pytorch->-r requirements.txt (line 5)) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch->reformer_pytorch->-r requirements.txt (line 5)) (3.10.0.2)\n",
      "Building wheels for collected packages: axial-positional-embedding, product-key-memory\n",
      "  Building wheel for axial-positional-embedding (setup.py): started\n",
      "  Building wheel for axial-positional-embedding (setup.py): finished with status 'done'\n",
      "  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2902 sha256=18d3a0fbb41cf559dda02e94b0bc3c2817fff371167cd1d3fbe4b0f36ea5dc68\n",
      "  Stored in directory: /root/.cache/pip/wheels/87/1e/cd/b5de135ee3faf0c4c525553227e601b0e7739264014dc3e98f\n",
      "  Building wheel for product-key-memory (setup.py): started\n",
      "  Building wheel for product-key-memory (setup.py): finished with status 'done'\n",
      "  Created wheel for product-key-memory: filename=product_key_memory-0.1.10-py3-none-any.whl size=3071 sha256=e53f75372fea4b6a6bbaee05d3ad5e8f0079f82fa357dc5e7a34efc69fa331c2\n",
      "  Stored in directory: /root/.cache/pip/wheels/a0/fc/1f/ecdffc3d221abee79cac215c986fb1913ae09c23e4705d86cc\n",
      "Successfully built axial-positional-embedding product-key-memory\n",
      "Installing collected packages: product-key-memory, local-attention, einops, axial-positional-embedding, reformer-pytorch\n",
      "Successfully installed axial-positional-embedding-0.2.1 einops-0.4.1 local-attention-1.4.4 product-key-memory-0.1.10 reformer-pytorch-1.4.4\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "2023-09-23 18:50:48,638 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"transformer-btc-1d-96-96-2023-09-23-18-40-32-313\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-291287855072/transformer-btc-1d-96-96-2023-09-23-18-40-32-313/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_aws\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_aws.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={}\n",
      "SM_USER_ENTRY_POINT=run_aws.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"train\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_MODULE_NAME=run_aws\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=8\n",
      "SM_NUM_GPUS=1\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-eu-west-1-291287855072/transformer-btc-1d-96-96-2023-09-23-18-40-32-313/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"transformer-btc-1d-96-96-2023-09-23-18-40-32-313\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-291287855072/transformer-btc-1d-96-96-2023-09-23-18-40-32-313/source/sourcedir.tar.gz\",\"module_name\":\"run_aws\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_aws.py\"}\n",
      "SM_USER_ARGS=[]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "Invoking script with the following command:\n",
      "/opt/conda/bin/python3.6 run_aws.py\n",
      "Args in experiment:\n",
      "Namespace(activation='gelu', batch_size=32, bucket_size=4, c_out=5, checkpoints='checkpoints/', d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='BTC_full_1day.csv', dec_in=5, des='Exp', devices='0,1,2,3', distil=True, do_predict=True, dropout=0.05, e_layers=2, embed='timeF', enc_in=5, factor=3, features='M', freq='d', gpu=0, is_training=1, itr=1, label_len=48, learning_rate=0.0001, loss='mse', lradj='type1', model='Transformer', model_id='BTC_96_96', moving_avg=25, n_hashes=4, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=96, root_path='/opt/ml/input/data/train', seq_len=96, sm_model_dir='/opt/ml/model', target='Close', train_epochs=1, use_amp=False, use_gpu=True, use_multi_gpu=False)\n",
      ">>>>>>> data rooth path : /opt/ml/input/data/train>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      ">>>>>>> data file path : BTC_full_1day.csv>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1431\n",
      "val 138\n",
      "test 368\n",
      "[2023-09-23 18:50:53.631 algo-1:55 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-09-23 18:50:53.677 algo-1:55 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "[2023-09-23 18:50:53.677 algo-1:55 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-09-23 18:50:53.678 algo-1:55 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-09-23 18:50:53.678 algo-1:55 INFO hook.py:255] Saving to /opt/ml/output/tensors\n",
      "[2023-09-23 18:50:53.679 algo-1:55 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-09-23 18:50:54.757 algo-1:55 INFO hook.py:591] name:enc_embedding.value_embedding.tokenConv.weight count_params:7680\n",
      "[2023-09-23 18:50:54.757 algo-1:55 INFO hook.py:591] name:enc_embedding.temporal_embedding.embed.weight count_params:1536\n",
      "[2023-09-23 18:50:54.757 algo-1:55 INFO hook.py:591] name:dec_embedding.value_embedding.tokenConv.weight count_params:7680\n",
      "[2023-09-23 18:50:54.757 algo-1:55 INFO hook.py:591] name:dec_embedding.temporal_embedding.embed.weight count_params:1536\n",
      "[2023-09-23 18:50:54.757 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.0.attention.query_projection.weight count_params:262144\n",
      "[2023-09-23 18:50:54.758 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.0.attention.query_projection.bias count_params:512\n",
      "[2023-09-23 18:50:54.758 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.0.attention.key_projection.weight count_params:262144\n",
      "[2023-09-23 18:50:54.758 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.0.attention.key_projection.bias count_params:512\n",
      "[2023-09-23 18:50:54.758 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.0.attention.value_projection.weight count_params:262144\n",
      "[2023-09-23 18:50:54.758 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.0.attention.value_projection.bias count_params:512\n",
      "[2023-09-23 18:50:54.758 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.0.attention.out_projection.weight count_params:262144\n",
      "[2023-09-23 18:50:54.758 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.0.attention.out_projection.bias count_params:512\n",
      "[2023-09-23 18:50:54.758 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.0.conv1.weight count_params:1048576\n",
      "[2023-09-23 18:50:54.758 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.0.conv1.bias count_params:2048\n",
      "[2023-09-23 18:50:54.758 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.0.conv2.weight count_params:1048576\n",
      "[2023-09-23 18:50:54.758 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.0.conv2.bias count_params:512\n",
      "[2023-09-23 18:50:54.759 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.0.norm1.weight count_params:512\n",
      "[2023-09-23 18:50:54.759 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.0.norm1.bias count_params:512\n",
      "[2023-09-23 18:50:54.759 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.0.norm2.weight count_params:512\n",
      "[2023-09-23 18:50:54.759 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.0.norm2.bias count_params:512\n",
      "[2023-09-23 18:50:54.759 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.1.attention.query_projection.weight count_params:262144\n",
      "[2023-09-23 18:50:54.759 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.1.attention.query_projection.bias count_params:512\n",
      "[2023-09-23 18:50:54.759 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.1.attention.key_projection.weight count_params:262144\n",
      "[2023-09-23 18:50:54.759 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.1.attention.key_projection.bias count_params:512\n",
      "[2023-09-23 18:50:54.759 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.1.attention.value_projection.weight count_params:262144\n",
      "[2023-09-23 18:50:54.759 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.1.attention.value_projection.bias count_params:512\n",
      "[2023-09-23 18:50:54.759 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.1.attention.out_projection.weight count_params:262144\n",
      "[2023-09-23 18:50:54.759 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.1.attention.out_projection.bias count_params:512\n",
      "[2023-09-23 18:50:54.760 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.1.conv1.weight count_params:1048576\n",
      "[2023-09-23 18:50:54.760 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.1.conv1.bias count_params:2048\n",
      "[2023-09-23 18:50:54.760 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.1.conv2.weight count_params:1048576\n",
      "[2023-09-23 18:50:54.760 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.1.conv2.bias count_params:512\n",
      "[2023-09-23 18:50:54.760 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.1.norm1.weight count_params:512\n",
      "[2023-09-23 18:50:54.760 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.1.norm1.bias count_params:512\n",
      "[2023-09-23 18:50:54.760 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.1.norm2.weight count_params:512\n",
      "[2023-09-23 18:50:54.760 algo-1:55 INFO hook.py:591] name:encoder.attn_layers.1.norm2.bias count_params:512\n",
      "[2023-09-23 18:50:54.760 algo-1:55 INFO hook.py:591] name:encoder.norm.weight count_params:512\n",
      "[2023-09-23 18:50:54.760 algo-1:55 INFO hook.py:591] name:encoder.norm.bias count_params:512\n",
      "[2023-09-23 18:50:54.760 algo-1:55 INFO hook.py:591] name:decoder.layers.0.self_attention.query_projection.weight count_params:262144\n",
      "[2023-09-23 18:50:54.761 algo-1:55 INFO hook.py:591] name:decoder.layers.0.self_attention.query_projection.bias count_params:512\n",
      "[2023-09-23 18:50:54.761 algo-1:55 INFO hook.py:591] name:decoder.layers.0.self_attention.key_projection.weight count_params:262144\n",
      "[2023-09-23 18:50:54.761 algo-1:55 INFO hook.py:591] name:decoder.layers.0.self_attention.key_projection.bias count_params:512\n",
      "[2023-09-23 18:50:54.761 algo-1:55 INFO hook.py:591] name:decoder.layers.0.self_attention.value_projection.weight count_params:262144\n",
      "[2023-09-23 18:50:54.761 algo-1:55 INFO hook.py:591] name:decoder.layers.0.self_attention.value_projection.bias count_params:512\n",
      "[2023-09-23 18:50:54.761 algo-1:55 INFO hook.py:591] name:decoder.layers.0.self_attention.out_projection.weight count_params:262144\n",
      "[2023-09-23 18:50:54.761 algo-1:55 INFO hook.py:591] name:decoder.layers.0.self_attention.out_projection.bias count_params:512\n",
      "[2023-09-23 18:50:54.761 algo-1:55 INFO hook.py:591] name:decoder.layers.0.cross_attention.query_projection.weight count_params:262144\n",
      "[2023-09-23 18:50:54.761 algo-1:55 INFO hook.py:591] name:decoder.layers.0.cross_attention.query_projection.bias count_params:512\n",
      "[2023-09-23 18:50:54.761 algo-1:55 INFO hook.py:591] name:decoder.layers.0.cross_attention.key_projection.weight count_params:262144\n",
      "[2023-09-23 18:50:54.761 algo-1:55 INFO hook.py:591] name:decoder.layers.0.cross_attention.key_projection.bias count_params:512\n",
      "[2023-09-23 18:50:54.761 algo-1:55 INFO hook.py:591] name:decoder.layers.0.cross_attention.value_projection.weight count_params:262144\n",
      "[2023-09-23 18:50:54.761 algo-1:55 INFO hook.py:591] name:decoder.layers.0.cross_attention.value_projection.bias count_params:512\n",
      "[2023-09-23 18:50:54.762 algo-1:55 INFO hook.py:591] name:decoder.layers.0.cross_attention.out_projection.weight count_params:262144\n",
      "[2023-09-23 18:50:54.762 algo-1:55 INFO hook.py:591] name:decoder.layers.0.cross_attention.out_projection.bias count_params:512\n",
      "[2023-09-23 18:50:54.762 algo-1:55 INFO hook.py:591] name:decoder.layers.0.conv1.weight count_params:1048576\n",
      "[2023-09-23 18:50:54.762 algo-1:55 INFO hook.py:591] name:decoder.layers.0.conv1.bias count_params:2048\n",
      "[2023-09-23 18:50:54.762 algo-1:55 INFO hook.py:591] name:decoder.layers.0.conv2.weight count_params:1048576\n",
      "[2023-09-23 18:50:54.762 algo-1:55 INFO hook.py:591] name:decoder.layers.0.conv2.bias count_params:512\n",
      "[2023-09-23 18:50:54.762 algo-1:55 INFO hook.py:591] name:decoder.layers.0.norm1.weight count_params:512\n",
      "[2023-09-23 18:50:54.762 algo-1:55 INFO hook.py:591] name:decoder.layers.0.norm1.bias count_params:512\n",
      "[2023-09-23 18:50:54.762 algo-1:55 INFO hook.py:591] name:decoder.layers.0.norm2.weight count_params:512\n",
      "[2023-09-23 18:50:54.762 algo-1:55 INFO hook.py:591] name:decoder.layers.0.norm2.bias count_params:512\n",
      "[2023-09-23 18:50:54.762 algo-1:55 INFO hook.py:591] name:decoder.layers.0.norm3.weight count_params:512\n",
      "[2023-09-23 18:50:54.762 algo-1:55 INFO hook.py:591] name:decoder.layers.0.norm3.bias count_params:512\n",
      "[2023-09-23 18:50:54.762 algo-1:55 INFO hook.py:591] name:decoder.norm.weight count_params:512\n",
      "[2023-09-23 18:50:54.763 algo-1:55 INFO hook.py:591] name:decoder.norm.bias count_params:512\n",
      "[2023-09-23 18:50:54.763 algo-1:55 INFO hook.py:591] name:decoder.projection.weight count_params:2560\n",
      "[2023-09-23 18:50:54.763 algo-1:55 INFO hook.py:591] name:decoder.projection.bias count_params:5\n",
      "[2023-09-23 18:50:54.763 algo-1:55 INFO hook.py:593] Total Trainable Params: 10531845\n",
      "[2023-09-23 18:50:54.763 algo-1:55 INFO hook.py:425] Monitoring the collections: losses\n",
      "[2023-09-23 18:50:54.767 algo-1:55 INFO hook.py:488] Hook is writing from the hook with pid: 55\n",
      "Epoch: 1 cost time: 4.918274879455566\n",
      "Epoch: 1, Steps: 44 | Train Loss: 0.3636263 Vali Loss: 0.4934958 Test Loss: 2.9321988\n",
      "Validation loss decreased (inf --> 0.493496).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 368\n",
      "test shape: (368, 96, 5) (368, 96, 5)\n",
      "test shape: (368, 96, 5) (368, 96, 5)\n",
      ">>>>>>>predicting : BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "pred 1\n",
      "2023-09-23 18:50:53,180 NumExpr defaulting to 8 threads.\n",
      "2023-09-23 18:50:53,210 setting is BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0\n",
      "2023-09-23 18:50:53,210 save model to /opt/ml/model/checkpoints/BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0\n",
      "2023-09-23 18:50:59,284 loading model from /opt/ml/model/checkpoints/BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0/checkpoint.pth\n",
      "2023-09-23 18:50:59,339 Testing: sm_model_dir is /opt/ml/model\n",
      "2023-09-23 18:50:59,339 Testing: Test result will be saved into folder /opt/ml/model/./test_results/BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0\n",
      "2023-09-23 18:50:59,654 Testing: testing on batch i=0\n",
      "2023-09-23 18:50:59,939 Testing: testing on batch i=1\n",
      "2023-09-23 18:50:59,957 Testing: testing on batch i=2\n",
      "2023-09-23 18:50:59,972 Testing: testing on batch i=3\n",
      "2023-09-23 18:50:59,990 Testing: testing on batch i=4\n",
      "2023-09-23 18:51:00,005 Testing: testing on batch i=5\n",
      "2023-09-23 18:51:00,021 Testing: testing on batch i=6\n",
      "2023-09-23 18:51:00,041 Testing: testing on batch i=7\n",
      "2023-09-23 18:51:00,057 Testing: testing on batch i=8\n",
      "2023-09-23 18:51:00,071 Testing: testing on batch i=9\n",
      "2023-09-23 18:51:00,086 Testing: testing on batch i=10\n",
      "2023-09-23 18:51:00,100 Testing: testing on batch i=11\n",
      "2023-09-23 18:51:00,191 Testing: Result will be saved into folder /opt/ml/model/./results/BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0\n",
      "2023-09-23 18:51:00,194 mse:2.975066900253296, mae:1.5653252601623535, rmse:1.7248382568359375, mape:2.2556591033935547, mspe:8.146798133850098\n",
      "2023-09-23 18:51:00,222 loading model from /opt/ml/model/checkpoints/BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0\n",
      "2023-09-23 18:51:00,569 Preding: preding on batch i=0\n",
      "2023-09-23 18:51:00,570 Preding: batch_y shape=torch.Size([1, 48, 5])\n",
      "Traceback (most recent call last):\n",
      "  File \"run_aws.py\", line 166, in <module>\n",
      "    main()\n",
      "  File \"run_aws.py\", line 137, in main\n",
      "    exp.predict(setting, True)\n",
      "  File \"/opt/ml/code/exp/exp_main.py\", line 327, in predict\n",
      "    trues = np.array(trues)\n",
      "ValueError: only one element tensors can be converted to Python scalars\n",
      "2023-09-23 18:51:01,485 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\n",
      "Command \"/opt/conda/bin/python3.6 run_aws.py\"\n",
      "2023-09-23 18:50:53,180 NumExpr defaulting to 8 threads.\n",
      "2023-09-23 18:50:53,210 setting is BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0\n",
      "2023-09-23 18:50:53,210 save model to /opt/ml/model/checkpoints/BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0\n",
      "2023-09-23 18:50:59,284 loading model from /opt/ml/model/checkpoints/BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0/checkpoint.pth\n",
      "2023-09-23 18:50:59,339 Testing: sm_model_dir is /opt/ml/model\n",
      "2023-09-23 18:50:59,339 Testing: Test result will be saved into folder /opt/ml/model/./test_results/BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0\n",
      "2023-09-23 18:50:59,654 Testing: testing on batch i=0\n",
      "2023-09-23 18:50:59,939 Testing: testing on batch i=1\n",
      "2023-09-23 18:50:59,957 Testing: testing on batch i=2\n",
      "2023-09-23 18:50:59,972 Testing: testing on batch i=3\n",
      "2023-09-23 18:50:59,990 Testing: testing on batch i=4\n",
      "2023-09-23 18:51:00,005 Testing: testing on batch i=5\n",
      "2023-09-23 18:51:00,021 Testing: testing on batch i=6\n",
      "2023-09-23 18:51:00,041 Testing: testing on batch i=7\n",
      "2023-09-23 18:51:00,057 Testing: testing on batch i=8\n",
      "2023-09-23 18:51:00,071 Testing: testing on batch i=9\n",
      "2023-09-23 18:51:00,086 Testing: testing on batch i=10\n",
      "2023-09-23 18:51:00,100 Testing: testing on batch i=11\n",
      "2023-09-23 18:51:00,191 Testing: Result will be saved into folder /opt/ml/model/./results/BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0\n",
      "2023-09-23 18:51:00,194 mse:2.975066900253296, mae:1.5653252601623535, rmse:1.7248382568359375, mape:2.2556591033935547, mspe:8.146798133850098\n",
      "2023-09-23 18:51:00,222 loading model from /opt/ml/model/checkpoints/BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0\n",
      "2023-09-23 18:51:00,569 Preding: preding on batch i=0\n",
      "2023-09-23 18:51:00,570 Preding: batch_y shape=torch.Size([1, 48, 5])\n",
      "Traceback (most recent call last):\n",
      "  File \"run_aws.py\", line 166, in <module>\n",
      "    main()\n",
      "  File \"run_aws.py\", line 137, in main\n",
      "    exp.predict(setting, True)\n",
      "  File \"/opt/ml/code/exp/exp_main.py\", line 327, in predict\n",
      "    trues = np.array(trues)\n",
      "ValueError: only one element tensors can be converted to Python scalars\n",
      "\n",
      "2023-09-23 18:51:25 Uploading - Uploading generated training model\n",
      "2023-09-23 18:51:25 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job transformer-btc-1d-96-96-2023-09-23-18-40-32-313: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python3.6 run_aws.py\"\n2023-09-23 18:50:53,180 NumExpr defaulting to 8 threads.\n2023-09-23 18:50:53,210 setting is BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0\n2023-09-23 18:50:53,210 save model to /opt/ml/model/checkpoints/BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0\n2023-09-23 18:50:59,284 loading model from /opt/ml/model/checkpoints/BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0/checkpoint.pth\n2023-09-23 18:50:59,339 Testing: sm_model_dir is /opt/ml/model\n2023-09-23 18:50:59,339 Testing: Test result will be saved into folder /opt/ml/model/./test_results/BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0\n2023-09-23 18:50:59,654 Testing: testing on batch i=0\n2023-09-23 18:50:59,939 Testing: testing on batch i=1\n2023-09-23 18:50:59,957 Testing: te",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m inputs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:s3_input_train}\n\u001b[1;32m     26\u001b[0m \u001b[39m#Start the training in the ephemeral remote compute \u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m estimator\u001b[39m.\u001b[39;49mfit(inputs, wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/model_train_aws_docker/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:311\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[39mreturn\u001b[39;00m context\n\u001b[1;32m    309\u001b[0m     \u001b[39mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m run_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/model_train_aws_docker/lib/python3.10/site-packages/sagemaker/estimator.py:1292\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjobs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1291\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m-> 1292\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlatest_training_job\u001b[39m.\u001b[39;49mwait(logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/model_train_aws_docker/lib/python3.10/site-packages/sagemaker/estimator.py:2474\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2472\u001b[0m \u001b[39m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2473\u001b[0m \u001b[39mif\u001b[39;00m logs \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 2474\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mlogs_for_job(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjob_name, wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, log_type\u001b[39m=\u001b[39;49mlogs)\n\u001b[1;32m   2475\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2476\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39mwait_for_job(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/model_train_aws_docker/lib/python3.10/site-packages/sagemaker/session.py:4849\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   4828\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlogs_for_job\u001b[39m(\u001b[39mself\u001b[39m, job_name, wait\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, poll\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, log_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAll\u001b[39m\u001b[39m\"\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   4829\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   4830\u001b[0m \n\u001b[1;32m   4831\u001b[0m \u001b[39m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4847\u001b[0m \u001b[39m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   4848\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4849\u001b[0m     _logs_for_job(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mboto_session, job_name, wait, poll, log_type, timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/model_train_aws_docker/lib/python3.10/site-packages/sagemaker/session.py:6760\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(boto_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   6757\u001b[0m             last_profiler_rule_statuses \u001b[39m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   6759\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m-> 6760\u001b[0m     _check_job_status(job_name, description, \u001b[39m\"\u001b[39;49m\u001b[39mTrainingJobStatus\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   6761\u001b[0m     \u001b[39mif\u001b[39;00m dot:\n\u001b[1;32m   6762\u001b[0m         \u001b[39mprint\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/model_train_aws_docker/lib/python3.10/site-packages/sagemaker/session.py:6813\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   6807\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCapacityError\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(reason):\n\u001b[1;32m   6808\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mCapacityError(\n\u001b[1;32m   6809\u001b[0m         message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   6810\u001b[0m         allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStopped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   6811\u001b[0m         actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   6812\u001b[0m     )\n\u001b[0;32m-> 6813\u001b[0m \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   6814\u001b[0m     message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   6815\u001b[0m     allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStopped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   6816\u001b[0m     actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   6817\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job transformer-btc-1d-96-96-2023-09-23-18-40-32-313: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python3.6 run_aws.py\"\n2023-09-23 18:50:53,180 NumExpr defaulting to 8 threads.\n2023-09-23 18:50:53,210 setting is BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0\n2023-09-23 18:50:53,210 save model to /opt/ml/model/checkpoints/BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0\n2023-09-23 18:50:59,284 loading model from /opt/ml/model/checkpoints/BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0/checkpoint.pth\n2023-09-23 18:50:59,339 Testing: sm_model_dir is /opt/ml/model\n2023-09-23 18:50:59,339 Testing: Test result will be saved into folder /opt/ml/model/./test_results/BTC_96_96_Transformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0\n2023-09-23 18:50:59,654 Testing: testing on batch i=0\n2023-09-23 18:50:59,939 Testing: testing on batch i=1\n2023-09-23 18:50:59,957 Testing: te"
     ]
    }
   ],
   "source": [
    "#Create the estimator object for PyTorch\n",
    "import os\n",
    "from sagemaker.pytorch.estimator import PyTorch # import PyTorch Estimator class \n",
    "\n",
    "estimator = PyTorch(\n",
    "    # Use the image created and pushed in the previous steps\n",
    "    # image_uri=custom_image_uri, #our custom pytorch image URI\n",
    "    entry_point = \"run_aws.py\", # training script\n",
    "    # Below two params are used with the default pytorch image that built by SageMaker\n",
    "    framework_version = \"1.8.1\", #PyTorch version\n",
    "    py_version = \"py3\", # Compatible Python version to use\n",
    "    instance_count = 1, #number of EC2 instances needed for training\n",
    "    # instance_type = \"ml.c5.xlarge\", #Type of EC2 instance/s needed for training\n",
    "    instance_type = \"ml.p3.2xlarge\", #Type of EC2 instance with GPU needed for training\n",
    "    disable_profiler = True, #Disable profiler, as it's not needed\n",
    "    role = execution_role, #Execution role used by training job\n",
    "    source_dir = \"./\", #Directory where training script is located\n",
    "    base_job_name='transformer-btc-1d-96-96', #Name of training job on AWS   \n",
    "    hyperparameters={}\n",
    ")\n",
    "\n",
    "s3_input_train = sagemaker.TrainingInput(s3_data=data_s3_uri, content_type='csv')\n",
    "\n",
    "inputs = {\"train\":s3_input_train}\n",
    "\n",
    "#Start the training in the ephemeral remote compute \n",
    "estimator.fit(inputs, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDos: \n",
    "1. Save the trained model into S3 bucket  \n",
    "    - Done by adding sm-model-dir in the parameter in run.py\n",
    "2. Use GPU to train \n",
    "    - Done by changing the estimator attribute to: instance_type = \"ml.p3.2xlarge\", #Type of EC2 instance with GPU needed for training\n",
    "3. Print out the test charts\n",
    "    - Done by updating folder_path params in exp_main.py\n",
    "4. Get BTC data running with Autoformer\n",
    "5. Set up early stop on epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy model with SageMaker\n",
    "# test model with SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize boto3 clients\n",
    "s3 = boto3.resource('s3')\n",
    "sagemaker = boto3.client('sagemaker')\n",
    "logs = boto3.client('logs')\n",
    "ecr = boto3.client('ecr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove S3 artifacts\n",
    "# bucket_name = 'sagemaker-eu-west-1-291287855072'\n",
    "# prefix = 'autoformer-training-job'  # Prefix to narrow down to specific files/directories\n",
    "# bucket = s3.Bucket(bucket_name)\n",
    "# for obj in bucket.objects.filter(Prefix=prefix):    \n",
    "#     pritn(\"Deleting S3 object: \"+obj.key)\n",
    "#     obj.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stop SageMaker notebook instances\n",
    "# notebook_instance_name = 'YOUR_NOTEBOOK_INSTANCE_NAME'\n",
    "# sagemaker.stop_notebook_instance(NotebookInstanceName=notebook_instance_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete CloudWatch logs\n",
    "# log_group_name = '/aws/sagemaker/TrainingJobs'\n",
    "# response = logs.describe_log_streams(logGroupName=log_group_name, orderBy='LastEventTime', descending=True)\n",
    "# for log_stream in response['logStreams']:\n",
    "#     if(log_stream['logStreamName'].startswith('autoformer-training-job')):\n",
    "#         print(\"Deleting log: \"+log_stream['logStreamName'])\n",
    "#         logs.delete_log_stream(logGroupName=log_group_name, logStreamName=log_stream['logStreamName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete Docker images from ECR\n",
    "# repository_name = 'hz_first_test'\n",
    "# images = ecr.list_images(repositoryName=repository_name)\n",
    "# for image in images.get('imageIds', []):\n",
    "#     if imageTag == \"latest\":\n",
    "#         # ecr.batch_delete_image(repositoryName=repository_name, imageIds=[image])\n",
    "#         print(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_train_aws_docker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
